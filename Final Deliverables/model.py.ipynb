{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1094b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad12061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - IMPORTING THE LIBRARIES\n",
    "\n",
    "# For matrix calculations and Data Management\n",
    "import numpy as np\n",
    "\n",
    "#Importing libraries required for the model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad, RMSprop\n",
    "from keras.applications import *\n",
    "from keras.preprocessing import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Activation, BatchNormalization, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for plotting charts used for data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Libtraries for Locating and loading data\n",
    "import glob\n",
    "from PIL import Image \n",
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9603cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - MAKING A LIST OF PATHS TO ALL FOLDERS WHERE WE HAVE OUR DATA\n",
    "\n",
    "# setting path to our dataset folder\n",
    "dirName=(\"augmented_data\")\n",
    "folders=listdir(dirName)\n",
    "#Getting the names for all the folders containing the data\n",
    "def getListOfFiles(dirName):\n",
    "    #create a list of sub directories and files (if any)\n",
    "    #names in the given directory\n",
    "    listOfFile=os.listdir(dirName)\n",
    "    allFiles=list()\n",
    "    for fol_name in listofFile:\n",
    "        fullPath=os.path.join(dirNmae, fol_name)\n",
    "        allFiles.append(fullPath)\n",
    "        return allFiles\n",
    "    Folders=getListOfFiles(dirName)\n",
    "    len(Folders)\n",
    "    subfolders=[]\n",
    "    for num in range(len(Folders)):\n",
    "        sub_fols=getListOfFiles(Folders[num])\n",
    "        subfolders+=sub_fols\n",
    "        #Now the subfolders contains the address to all our data folders for each class\n",
    "        subfolders\n",
    "# STEP 2(i)- LOADING THE IMAGES INTO THE MACHINE UNDERSTANDABLE DATA\n",
    "        # Loading the data and preprocessing it to make it in trainable format\n",
    "        # X data includes the data generated for each image \n",
    "        # Y data includes ID No, unique for every different species, so we have 6 classes\n",
    "        # which will the label\n",
    "        X_data=[]\n",
    "        Y_data=[]\n",
    "        id_no=0\n",
    "        # to make a list of tuples, where we'll store the info about the image, category and species\n",
    "        found=[]\n",
    "        # iterating in all folders under Augmneted data folder\n",
    "        for paths in subfolders:\n",
    "            # set folder path for each unique class and category\n",
    "            files=glob.glob(paths+\"/*.jpg\")\n",
    "            # adding tuples to the list that contains folder name and subfolder name\n",
    "            found.append((paths.split('\\\\')[-2],paths.spilt('\\\\')[-1]))\n",
    "            # iterating all the files under the folder one by one\n",
    "            for myFile in files:\n",
    "                img=Image.open(myFile)\n",
    "                img=img.resize((224,224), Image.ANTIALIAS) #resizes image without ratio\n",
    "                # converts the image to numpy arrays\n",
    "                img=np.array(img)\n",
    "                if img.shape==(224,224,3): \n",
    "                    # add the numpy images to the matrix with all the data\n",
    "                    X_data.append(img)\n",
    "                    Y_data.append(id_no)\n",
    "                    id_no+=1\n",
    "# STEP 2(ii) - DATA SPLITTING INTO THE TRAIN AND TEST to see our data \n",
    "                    print(X_data)\n",
    "                    print(Y_data)\n",
    "                    # Converting lists to np arrays again\n",
    "                    X=np.arrays(X_data)\n",
    "                    Y=np.arrays(Y_data)\n",
    "                    # Print shapes to see if they are correct\n",
    "                    print(\"x-shape\",X.shape,\"y-shape\", Y.shape)\n",
    "                    X=X.astype('float32')/255.0\n",
    "                    # keras library offers a function called to_categorical() or OneHotEncoder\n",
    "                    #integer data. The sequence has an example of all known values so we use the to_categorical() function directly\n",
    "                    y_cat=to_categorical(Y_data,len(subfolders))\n",
    "                    print(\"X shape\",X,\"y_cat shape\",y_cat)\n",
    "                    print(\"X shape\",X.shape,\"y_cat shape\", y_cat.shape)\n",
    "                    X_train, X_test, y_train, y_test=tain_test_split(X,y_cat,test_size=0.2)\n",
    "                    print(\"The model has\"+str(len(X_train))+\"inputs\")\n",
    "                    \n",
    "# STEP 3 - BUILDING A CONVOLUTION NEURAL NETWORKS\n",
    "                    \n",
    "                    # STEP 3(i) - Sequential\n",
    "                    \n",
    "                    early_stop_loss=EarlyStopping(monitor='loss',patience=3,verbose=1)\n",
    "                    early_stop_val_acc=EarlyStopping(monitor='val_accuracy',patience=3,verbose=1)\n",
    "                    model_callbacks=[early_stop_loss,early_stop_val_acc]\n",
    "                    \n",
    "                    # STEP 3(ii) ADDING LAYERS (Conv, Maxpool, Flatten, Dense, Dropout)\n",
    "                    \n",
    "                    # defining our model, All the layers and configurations\n",
    "                    \n",
    "                    def load_CNN(output_size):\n",
    "                        K.clear_session()\n",
    "                        model=Sequential()\n",
    "                        model.add(Dropout(0.4,input_shape=(224,224,3)))\n",
    "                        model.add(Conv2D(256,(5,5),input_shape=(224, 224, 3), activation='relu'))\n",
    "                        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "                        \n",
    "                        #model.add(BatchNormalization())\n",
    "                        model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "                        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "                        #model.add(BatchNormalization())\n",
    "                        model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "                        model.add(MaxPool2D(pool_size=(2,2)))\n",
    "                        #model.add(BatchNormalization())\n",
    "                        model.add(Flatten())\n",
    "                        model.add(Dense(512, activation='relu'))\n",
    "                        model.add(Dropout(0.3))\n",
    "                        model.add(Dense(256, activation='relu'))\n",
    "                        model.add(Dropout(0.3))\n",
    "                        model.add(Dense(128, activation='relu'))\n",
    "                        model.add(Dropout(0.3))\n",
    "                        model.add(Dense(output_size, activation='softmax'))\n",
    "                        return model\n",
    "                    \n",
    "# STEP 4- BUILDING MODEL SUMMARY\n",
    "\n",
    "                    # STEP 4(i) - Building a model based on the above defined function\n",
    "                    \n",
    "                    model=load_CNN(6) # Number of Columns/ Outputs\n",
    "                    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "                    model.summary() # to print the model summary\n",
    "                    weights=model.get_weights() # to get the weights from the created model\n",
    "                    \n",
    "                    # STEP 4(ii) - FITTING THE MODEL\n",
    "                    # some arrays to store the result of each model (model trained on each bath size)\n",
    "                    \n",
    "                    histories_acc=[]\n",
    "                    histories_val_acc=[]\n",
    "                    histories_loss=[]\n",
    "                    histories_val_loss=[]\n",
    "                    model.set_weights(weights)\n",
    "                    h=model.fit(X_train,y_train,\n",
    "                                batch_size=16,\n",
    "                                epochs=7,\n",
    "                                verbose=1,\n",
    "                                callbacks=[early_stop_loss],\n",
    "                                shuffle=True,\n",
    "                                validation_dat=(X_test, y_test))\n",
    "                    model.summary()\n",
    "# STEP 5 - EVALUATION: (i) ACCURACY\n",
    "                   \n",
    "                    # Printing the keys we have for the stores values\n",
    "                    print(h.history.keys())\n",
    "                    \n",
    "                    # Appendind the data for each epochh in an arr and for each batch size\n",
    "                    \n",
    "                    histories_acc.append(h.history['acc'])\n",
    "                    histories_val_acc.append(h.history['val_acc'])\n",
    "                    histories_loss.append(h.history['loss'])\n",
    "                    histories_val_loss.append(h.history['val_loss'])\n",
    "                    \n",
    "                    # Converting into numpy arrays\n",
    "                    \n",
    "                    histories_acc=np.array(histories_acc)\n",
    "                    histories_val_acc=np.array(histories_val_acc)\n",
    "                    histories_loss=np.array(histories_loss)\n",
    "                    histories_val_loss=np.array(histories_val_loss)\n",
    "                    \n",
    "                    # Here we have 3 columns and 6 rows each, ever row represents different batch sizes and every coloumn represents different epoch scores\n",
    "                    print('histories_acc',histories_acc,\n",
    "                           'histories_loss',histories_loss,\n",
    "                           'histories_val_acc',histories_val_acc,\n",
    "                           'histories_val_loss',histories_val_loss)\n",
    "                    \n",
    "# PREDICTION\n",
    "# STEP 5(ii) - LOADING A TEST IMAGE AND MAKING A TEST PREDICTION\n",
    "                    \n",
    "    # Predicting the image's classes\n",
    "    # Individual scores for each calss as well as class with the highest score is printed \n",
    "    # Making predictions, storing teh result as an array of probabilities of each class predicted\n",
    "                    \n",
    "                    predictions=model.predict_proba([X_test[image_number].reshape(1,224,224,3)])\n",
    "                    for idx, result, x in zip(range(0,6),found, predictions[0]):\n",
    "                        print(\"Label: {}, Type : {}, Species : {}, Score : {}%\".format(idx,result[0],result[1], round(x*100,3)))\n",
    "                        #predictiong the class with max probabilty\n",
    "                        ClassIndex=model.predict_classes([X_test[image_number].reshape(1,224,224,3)])\n",
    "                        #getting the index of th eclass which we can pass to the boat_types list to get the boat type name\n",
    "                        ClassIndex\n",
    "                        #printing the final output\n",
    "                        print(found[ClassIndex[0]])\n",
    "                        # Loading the test data\n",
    "                        image_number =np.random.randint(0,len(X_test))\n",
    "                        print(image_number)\n",
    "                        # Plotting the test image\n",
    "                        plt.figure(figsize=(8,8))\n",
    "                        plt.imshow(X_test[image_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e849e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# STEP 6 - SAVE THE MODEL\n",
    "                        \n",
    "model_json=model.to_json() #indent=2\n",
    "with open(\"final_model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "    # serializing the weights to H5\n",
    "    \n",
    "    model.save_weights(\"final_model.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
